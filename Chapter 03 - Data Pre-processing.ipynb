{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03 - Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data pre-processing: e.g. addition, deletion, or transformation of training set data\n",
    "* need is determined by the type of model\n",
    "* _unsupervised_ vs. _supervised_ approaches to data processing: whether the outcome variable is considered (supervised) or not (unsupervised)\n",
    "* _feature engineering_: how predictors are encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Data Transformations for Individual Predictors\n",
    "\n",
    "### Centering and Scaling\n",
    "\n",
    "* **center** a predictor by subtracting the mean value of the predictor from all values\n",
    "    * results in a mean of '0'\n",
    "* **scale** a predictor by dividing each value by the predictor's standard deviation\n",
    "    * results in a standard deviations of '1'\n",
    "* benefit: improves numerical stability for some calculations\n",
    "* negative: lose some interpretability in that values are no longer in \"original\" units\n",
    "\n",
    "### Transformations to Resolve Skewness\n",
    "\n",
    "* un-skewed: distribution is roughly symmetric; probablility of falling on each side of mean is roughly equal\n",
    "* right-skewed: distribution has a large number of points on the left side of the distribution\n",
    "* left-skewed: sim.\n",
    "* skewness statistic:\n",
    "\n",
    "\\begin{equation}\n",
    "skewness = \\frac{\\sum{(x_i - \\bar{x})^3}}{(n - 1)v^{3/2} }\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\\begin{equation}\n",
    "v = \\frac{\\sum{(x_i - \\bar{x})^2}}{(n-1)}\n",
    "\\end{equation}\n",
    "\n",
    "* if symmetrics, skewness statistics is close to 0\n",
    "* right skewed generates larger, positive values\n",
    "* left skewed generates smaller, negative values\n",
    "\n",
    "Some possible approaches to remove skewness:\n",
    "\n",
    "* replace data with log, square root, or inverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Empirical Identification of Transformation\n",
    "\n",
    "* determine $\\lambda$ parameter via maximum likelihood estimation\n",
    "\n",
    "\\begin{equation}\n",
    "    x =\n",
    "    \\begin{cases}\n",
    "      \\frac{x^\\lambda - 1}{\\lambda{}}, & \\text{if}\\ \\lambda{} \\ne 0 \\\\\n",
    "      \\log{x}, & \\text{if}\\ \\lambda{} = 0\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "* only if all $x > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data Transformations for Multiple Predictors\n",
    "\n",
    "### Outliers\n",
    "\n",
    "* are any \"scientifically\" invalid (e.g. negative blood pressure, etc.)?\n",
    "* are any due to data recording errors?\n",
    "* care must be taken not to hastily remove or change\n",
    "    * in small smaples, they may be an indication of skewed data\n",
    "    * they may be a special part of the population under study, or a different population altogether\n",
    "* some models are resistant to outliers: e.g. trees, classification SVMs\n",
    "* potential transform to resolve outliers: **spatial sign**\n",
    "    * projects predictor values onto a multidimensional sphere\n",
    "    * makes all samples the same distance from the center of that sphere\n",
    "    \n",
    "\\begin{equation}\n",
    "x^*_{ij} = \\frac{ x_{ij}  }{  \\sqrt{  \\sum_{j = 1}^{P}{x_{ij}^2}  }  }\n",
    "\\end{equation}\n",
    "\n",
    "* important to center and scale predictor values before using spatial sign transform\n",
    "* removing predictors after transforming with spatial sign may be problematic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Reduction and Feature Extraction\n",
    "\n",
    "* generate smaller number of predictors that attempt to capture the majority of the information in the original variables\n",
    "* \"signal extraction\" or \"feature extraction\"\n",
    "\n",
    "#### PCA\n",
    "\n",
    "* Seeks to find **Principal Components**: linear combinations of the predictors capturing the most possible variance\n",
    "* the 1st PC captures the most variances\n",
    "* the next PC captures the most remaining variance AND is _uncorrelated_ with all previous PCs\n",
    "* and so on...\n",
    "\n",
    "\\begin{equation}\n",
    "PC_j = (a_{j1} \\times \\text{Predictor 1}) + (a_{j2} \\times \\text{Predictor 2}) + \\dots + (a_{jP} \\times \\text{Predictor P})\n",
    "\\end{equation}\n",
    "\n",
    "* primary advantage: PCs are _uncorrelated_\n",
    "* BUT, PCA can also generate components that summarize irrelevant characteristics of the data\n",
    "* because PC seeks high variability, it's influenced by scale & skewness\n",
    "    * pre-PCA, transofrm skewed predictors\n",
    "    * pre-PCA, center and scale\n",
    "* if predcitor -> response relationship is not connected to predictors' variability, consider a supervised technique (e.g. PLS) rather than an unsupervised technique like PCA\n",
    "* to determine the number of PCs to retain, use a scree plot\n",
    "    * select component number immediately before variation tapers off\n",
    "    * alternative approach is to use cross-validation to identify optimal cutoff\n",
    "* visually examining PCs\n",
    "    * plot PCs against each other\n",
    "    * use symbology to indicate classes/groupings\n",
    "    * might demonstrate clusters or outliers\n",
    "    * might demonstrate potential separation of classes\n",
    "    * might show significant overlap if no clear separation\n",
    "    * use care in setting the scale used to visualize\n",
    "        * avoid over-emphasis of variation in later PCs\n",
    "* loadings can provide details about predictor contribution to PCs\n",
    "    * loadings closer to 0 indicate a lower contribution of that predictor into the PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.4 Dealing with Missing Values\n",
    "\n",
    "* _structurally missing_: (e.g. number of children a man has given birth to)\n",
    "* or, value cannot or was not determined at time of model building\n",
    "* important to understand _why_ values are missing\n",
    "    * is pattern of missing data related to outcome? \n",
    "    * **informative missingness**; can introduce significant bias in the model\n",
    "    * \"Napoleon Dynamite Effect\": majority of customer ratings were at extremes (positive and negative) - only those with strong opinions provided ratings\n",
    "    * **censored data**: exact value is missing, but _something_ is known of its value\n",
    "    * for predictive modeling, common to treat censored data either as missing data or as the observed quantity\n",
    "* missing data tends to be related to predictor variables rather than the samples, but could be either\n",
    "    * if isolated to specific predictors, these could be removed if percentage missing is high enough\n",
    "    * if concentrated in specific samples:\n",
    "        * in large datasets, removal of these samples may be un-problematic\n",
    "        * for smaller sets, cost of removal is higher\n",
    "* some modeling approaches can handle missing data: (e.g. trees, etc.)\n",
    "* for others, consider **imputation** of missing data:\n",
    "    * effectively predicting missing _predictors_ from other predictor values\n",
    "    * adds uncertainty\n",
    "    * if using resampling to select tuning parameter values or estimate performance, imputation should be incorporated within the resampling\n",
    "* are there correlations between a predictor with missing values and other predictors with few missing values?\n",
    "* another approach is to use K-nearest neighbors to find \"nearby points\" without missing values to be used for imputation\n",
    "    * one advantage of this approach is that the imputed values are confined to the range of the training set values\n",
    "    * one disadvantage is that it requires the entire training set to impute a missing value\n",
    "    * $K$ is a tuning parameter, as is the distance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Removing Predictors\n",
    "\n",
    "* fewer predictors means decreased computational time and complexity\n",
    "* in case of correlated predictors, removing one should maintain similar performance, while improving simplicity and interpretability\n",
    "* zero variance predictors (i.e. with a single value) can cause problems, and can easily be discarded\n",
    "* near-zero variance predictors: predictors that have only a handful of unique values that occur with very low frequencies:\n",
    "    * the fraction of unique values over the sample size is low (e.g. 10%)\n",
    "    * the ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say around 20)\n",
    "    * if both are true, and model is susceptible to this type of predictor, consider removing the variable    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Between-Predictor Correlations\n",
    "\n",
    "* **Collinearity**: when pair of predictors have substantial correlation\n",
    "* **Multicollinearity**: collinearity between multiple predictors\n",
    "* Correlation matrix: visual technique for identifying presence of collinearity\n",
    "* PCA can be used as a non-visual technique to characterize the magnitude of collinearity\n",
    "    * e.g. if first PC accounts for large percentage of variance, this implies there is at least one group of predictors that represent the same information\n",
    "    * loadings can be used to identify _which_ predictors are within those groups\n",
    "* Reasons to avoid collinearity:\n",
    "    * redundant predictors add more complexity\n",
    "    * for some techniques (e.g. linear regression) they can result in unstable models, numerical errors, and degraded predictive performance\n",
    "* variance inflation factor (VIF) can be used to identify predictors that are impacted in the context of linear regression\n",
    "    * drawbacks of using outside linear regression:\n",
    "        * developed for linear models\n",
    "        * requires # samples > # predictors\n",
    "        * doesn't indicate which predictors should be removed\n",
    "        \n",
    "Heuristic for dealing with multicollinearity: Remove the minimum number of predictors to ensure that all pairwise correlations are below a certain threshold:\n",
    "\n",
    "* calculate correlation matrix\n",
    "* identify largest absolute, pairwise correlation\n",
    "* for the two predictors identified, determine which has the largest correlation with all _other_ predictors\n",
    "* remove the predictor with the larger avg correlation\n",
    "* iterate until there are no absolute pariwise correlations above the threshold\n",
    "\n",
    "PCA (and other feature extraction methods) can also be used in response to the presence of collinearities, at the cost of adding complexity to the predictor-response relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Adding Predictors\n",
    "\n",
    "* Categoricals: encoded as **dummy variables**\n",
    "    * each category gets its own bit variable (0/1)\n",
    "    * for regression, n-1 (n = # of categories) variables are included, to avoid numerical issues\n",
    "    * for techniques not affected by this, including n variables helps from an interpretability standpoint\n",
    "* In some cases, adding predictors can help the predictive performance of a simpler modeling technique, avoiding the need to use a more complex technique.\n",
    "    * e.g. Adding non-linear (i.e. non-first-order) terms to a linear regression model.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Binning Predictors\n",
    "\n",
    "* Avoid manual binning of numeric values:\n",
    "  * can lead to loss of performance of model\n",
    "  * loss of precision in the predictions\n",
    "  * can lead to high rate of false positives\n",
    "* Note: this is in contrast to _automatic_ binning that happens as part of the algorithmic processing of some techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
